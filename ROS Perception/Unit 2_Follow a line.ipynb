{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2: Follow a line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this unit you will learn how to start using the most basic and also the most poverfull tool for perception in ROS: OpenCV. [**OpenCV**](https://docs.opencv.org/master/d9/df8/tutorial_root.html) is the most extensive and complet library for image recognition. With it you will be able to work with image like never before, applying filters, postprocessing, and working wiht images in any way you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use OpenCV in ROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have gussed OpenCV is not a ROS library but its been integrated nicelz into with [**OpenCV_birdge**](http://wiki.ros.org/cv_bridge). This package allows the ROS imaging topics use the OpenCV image variable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, OpenCV images come in **BGR** image format while  regular ROS images are in the more standard RGB encoding. OpenCV_bridge provides a nice feature to convert between them. Also there are many other functions to transfer images to openCV variables transparently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn how to use OpenCV you will use the RGB camera of this turtlebot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C5.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this turtlebot is in a very strange environment. On the floor there is a Yellow path painted and some stars of different colours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C6.png\" width=\"380\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you will have to do is to make the robot move in the environment following the yellow line. For that we will divide the work in the following phases:\n",
    "* Get images from ROS topic and convert into OpenCV format\n",
    "* Process the Images using OpenCV libraries to obtain the data we want for the task\n",
    "* Move the robot along the yellow line, based on the data obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Images from a ROS topic and show them with OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing anything, create a new package called **my_following_line_package** with dependency in *rospy*, and also a launch and scripts directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step will be to get the images from a ROS topic, put them into the OpenCV format and show them in the Graphical Tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have an example of how to do it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> line_follower_basics.py </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import roslib\n",
    "import sys\n",
    "import rospy\n",
    "import cv2\n",
    "import numpy as np\n",
    "from cv_bridge import CvBridgeError\n",
    "from geometry_msgs.msg import Twist\n",
    "from sensor_msgs.msg import Image\n",
    "\n",
    "class LineFollower(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.bridge_object = CvBridge()\n",
    "        self.image_sub = rospy.Subscriber(\"/camera/rgb/image_raw\", Image, self.camera_callback)\n",
    "        \n",
    "    def camera_callback(self,data):\n",
    "        \n",
    "        try:\n",
    "            #we selec bgr8 because its the OpenCV encoding by default\n",
    "            cv_image = self.bridge_object.imgmsg_to_cv2(data, desired_encoding = \"bgr8\")\n",
    "        except CvBridgeError as e:\n",
    "            print(e)\n",
    "            \n",
    "        cv2.imshow(\"Image window\", cv_image)\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "def main():\n",
    "    line_follower_object = LineFollower()\n",
    "    rospy.init_node('line_following_node', anonymous=True)\n",
    "    try:\n",
    "        rospy.spin()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Shutting down\")\n",
    "    cv2.destoryAllWindows()\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> END line_follower_basics.py </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are serval elements to comment on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from cv_bridge import CvBridgeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This imports are the basic necessary to work with images in ROS.\n",
    "<br>\n",
    "You have OpenCVs library(cv2). Why the 2? Beacuase there is already a version 3.\n",
    "<br>\n",
    "You have the numpy library which make matrix and other operations easy to work with. CV_Bridge, which allows ROS work with OpenCV easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.image_sub = rospy.Subscriber(\"/camera/rgb/image_raw\", Image, self.camera_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This susbcriber to the image topic. This topic publishes information of type **sensor_msgs/image**. Execute the following command to see what all the different variables inside this message type:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rosmsg show sensor_msgs/Image"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rostopic echo -n1 /camera/rgb/image_raw/height"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rostopic echo -n1 /camera/rgb/image_raw/width"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rostopic echo -n1 /camera/rgb/image_raw/encoding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rostopic echo -n1 /camera/rgb/image_raw/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important infomation here is:\n",
    "* height and width: These are the dimensions in pixels of the camera. \n",
    "* encoding: How these pixels are encoded. This means what will each value in the data array mean. In this case its **rgb8**. This means that the values in data will be a color value represented as a red/green/blue in 8-bit integers.\n",
    "* data: The Image data.\n",
    "<br>\n",
    "\n",
    "If you want the full docunmentation of this class, please reffer to: http://docs.ros.org/melodic/api/sensor_msgs/html/msg/Image.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the cv_bridge package, we can easily convert the image data contained in a ImageSensorData into a format that OpenCV understands. By converting it into OpenCV, we can use all the power of library to process the images of the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #we selec bgr8 because its the OpenCV encoding by default\n",
    "    cv_image = self.bridge_object.imgmsg_to_cv2(data, desired_encoding = \"bgr8\")\n",
    "except CvBridgeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the image from a ROS topic and store it in an OpenCV variable. The var data is the one that contains a ROS message with the image captures by camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Image window\", cv_image)\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will open a gui where you can see the contents of the variable \"cv_image\". This is essential afterwards to see the effects fo the different filters and cropping of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destoryAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will close all the image windows when the program is terminated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executing this program you will have to see the image by clicking on the **Graphical Tools** icon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> Exercise U2-1 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now you just have to create a python script with the command code and see if it works.\n",
    "<br>\n",
    "Create a package for the ocasion called \"my_follow_line_package\", and put that python file in a scripts folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C7.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then launch the code using the command below, and test that it actually works."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rosrun my_follow_line_package line_follower_basics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> END  Exercise U2-1  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C8.png\" width=\"380\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply Filter To the Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw image is useless unless you filter it to only see the colour you want to track and you crop the parts os the image you are not interested in. This is to make your progarm faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to extract some data from the images in a way that we can move the robot to follow the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Step: Get Image Info and Crop the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start using the images for detecting things, you must take into account two things:\n",
    "* One of the most basic data you need to work with images is the dimensions. Is it 800x600? 1200x1024? 60x60? This is crucial to position the elements detected in the image.\n",
    "* And second is cropping the image. Its very important to work as soon as possible with the minimum size of image required for the task. This makes the detecting system much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get image dimensions and crop the parts of the image we dont need\n",
    "# Bare in mind that because its image matrix first value is start and second value is down limit.\n",
    "# Select the limits so that it gets the line not to close, not to far and the minimum portion possible\n",
    "# To make process faster.\n",
    "height, width, channels = cv_image.shape\n",
    "descentre = 160\n",
    "rows_to_watch = 20\n",
    "crop_img = cv_image[(height)/2+descentre:(height)/2+(descentre+rows_to_watch)][1:width]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why this values and not other values? Well it depends on the task. In this case you are interested in lines that aren't too far away from the robot, nor to near. If you concertrate on the lines too far away it wont follow the lines, but it will just go across the map. On the other hand concentrating on lines too close won't give the robot time to adapt to chanches in the line.\n",
    "<br>\n",
    "Its also vital to ptimise the region of image result of croping. If its too big, too much data will be processed making your program too slow. On the other hand it has to have enough image to work with. \n",
    "<br>\n",
    "At the end you will have to adapt it to each situation.\n",
    "<br>\n",
    "Do **Exercise U2.3** to test the effects of different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second step: COnvert from BGR to HSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remmember that OpenCV works with BGR instead of RGB, for historical reasons (some cameras on the days when OpenCV was created worked with BGR).\n",
    "<br>\n",
    "Well it seems that it not very easy to work with neither RGB nor BGR to differenciate colours. Thats why HSV is used. The ides behind HSV is to remove the component of colour saturation. This way its easier to recognise the same color in different light conditions, which is a serious issue in image recognition. More infromation:https://en.wikipedia.org/wiki/HSL_and_HSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C9.png\" width=\"380\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert from RGB to HSV\n",
    "hsv = cv2.cvtColor(crop_img, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# Define the Yellow Colour in HSV\n",
    "#RGB\n",
    "# [[[222,255,0]]]\n",
    "#BGR\n",
    "# [[[0,255,222]]]\n",
    "'''\n",
    "To know which color to track in HSV, Put in BGR. Use ColorZilla to get the color registered by the camera\n",
    ">>> yellow = np.uint8([[[B, G, R]]])\n",
    ">>> hsv_yellow = cv2.cvtColor(yellow, cv2.COLOR_BGR2HSV)\n",
    ">>> print( hsv_yellow )\n",
    "[[[[ 34 255 255]]]]\n",
    "'''\n",
    "\n",
    "lower_yellow = np.array([20, 100, 100])\n",
    "upper_yellow = np.array([50, 255, 255])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this piece of code you are converting the croped_image(crop_img) into HSV.\n",
    "<br>\n",
    "Then you select which colour in HSV you want to track, that would be selecting from the base of the colour cone, a single point. Because HSV values are quite difficult to generate, its better that you use a colour picker tool like ***ColorZilla*** to pick the RGB coding of colour to track, in this case the yellow line in the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you havre it, use the example code given in python terminal or create a tiny program that uses numpy as np and cv2 to convert it to HSV. In the example given the colour of the line is HSV=[[[34 255 255]]]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have it then you have to select an upper and lower bound to consider the region of the cone base that you will consider Yellow. Bigger the refion more gradients of your picked colour will be accepted. This will depend on how your robot detects and the colour variations in the image and how critical is mixing similar colours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third step: Apply the mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to generate a version of the cropped image in  which you only see  two colours: black and white. The white will be all the colours you consider yellow and the rest will be black. Its a binary image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you need to do this? It has two functions:\n",
    "* Doing this you dont have a continuous detection. It IS the Colour or its NOT, there is no in between.  This is vital for the centroid calculation that will be done after, because it only works on the principal of YES or NO.\n",
    "* Second, it will allow afterwards generate the Result image in which you extract everything on the image except the colour line, seeing only what you are interested on seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold the HSV image to get only yellow colors\n",
    "mask = cv2.inRange(hsv, lower_yellow, upper_yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bitwise-AND mask and original image\n",
    "res = cv2.bitwise_and(crop_img, crop_img, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You then merge the croped coloured image in HSV with the binary mask image, to colour only the detections, leaving the rest in black."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth step: Get The Cetroids, draw a circle where the centroid is and show all  the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centroids is essence represent points in space in which mass concentrates, the center of mass. Centroid and centers of mass are the same thing as far as this course goes. And they are calculated using Integrals.\n",
    "<br>\n",
    "This is extrapolated into images. But instead of having mass, we have colour. The place where you have more of the colour you are looking for, then that's where the centroid will be. Its the center of the blobs seen in an image.\n",
    "<br>\n",
    "Thats why you applied the mask to make the image binary. This way you can calculate eassily where the center of mass is located. This is because its a describte function, not a continuos one. This means that it allows us to integrate in a discrete fasion and not need a function discrbing the fluctuations in cuantity of colour throughout the region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These centrois are vital in blob tracking because they give you a precise point in space where the blob is. This you will use it to follow the blob and therefore follow the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is needed to calculate the centroids of the colour blobs. You use the image moments for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1e040546b1fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# calculate centroid of the blob of binary image using ImageMoments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'm10'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'm00'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'm01'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'm00'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mZeroDivisionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# calculate centroid of the blob of binary image using ImageMoments\n",
    "m = cv2.moments(mask, Flase)\n",
    "try:\n",
    "    cx, cy = m['m10']/m['m00'], m['m01']/m['m00']\n",
    "except ZeroDivisionError:\n",
    "    cx, cy = height/2, width/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here, you will obtain the coordinates of the cropped image where the centroid of the positive colour yellow detection occur. If nothing is detected it will be positioned in the center of the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want a more detailed explaination an OpenCV exercises in all that can be obtained with contour features: https://docs.opencv.org/3.1.0/dd/d49/tutorial_py_contour_features.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in all the mathematical justification: https://en.wikipedia.org/wiki/Image_moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the centroid in the resultut image\n",
    "# cv2.cricle(img, center, radius, color [, thickness[, lineType[, shift]]])\n",
    "cv2.cricle(res, (int(cx), int(cy)), 10, (0, 0, 255), -1) #BGR\n",
    "\n",
    "cv2.imshow(\"Original\", cv_image)\n",
    "cv2.imshow(\"HSV\", hsv)\n",
    "cv2.imshow(\"MASK\", mask)\n",
    "cv2.imshow(\"RES\", res)\n",
    "\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV allows you to draw a lot of things over the images, not only geometric shapes. But in this case a cricle will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.cricle(res, (centre_cricle_x, centre_cricle_y), LineWidth, (RGBColour of line), TypeOfLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using this feature to draw a cricle in the location of the calculated centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C10.png\" width=\"380\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Info: https://docs.opencv.org/3.0-beta/modules/imgproc/doc/drawing_functions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Move the TurtleBot based on the position of the Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_x = cx - width/2;\n",
    "twist_object = Twist()\n",
    "twist_object.linear.x = 0.2;\n",
    "twist_object.angular.z = -error_x / 100;\n",
    "rospy.loginfo(\"ANGULAR VALUE SENT===>\" + str(twist_object.angular.z))\n",
    "\n",
    "# Make it start turning\n",
    "self.movekobuki_object.move_robot(twist_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This control is pased on a Proportional control. This means that i will oscilate a lot and probably have an error. But is the simples way of moving the robot and has the job done.\n",
    "<br>\n",
    "It gives always a constant linear movement and the angular Z velocity depends on the difference between the centroid center in X and the center of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To move the robot you can use this module to avoid some topic related problems when comunicating with Kobuki, like loosing first topics or similar issues:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    move_robot.py \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "from geometry_msgs.msg import Twist\n",
    "\n",
    "class Move Kobuki(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cmd_vel_pub = rospy.Publisher('cmd_vel', Twist, queue_size=10)\n",
    "        self.cmd_vel_subs = rospy.Subscriber('/cmd_vel', Twist, self.cmdvel_callback)\n",
    "        self.last_cmdvel_command = Twist()\n",
    "        self._cmdvel_pub_rate = rospy.Rate(10)\n",
    "        self.shutdown_detected = False\n",
    "        \n",
    "    def cmdvel_callback(self, msg):\n",
    "        self.last_cmdvel_command = msg\n",
    "        \n",
    "    def compare_twist_commands(self, twist1, twist2):\n",
    "        LX = twist1.linear.x == twist2.linear.x\n",
    "        LY = twist1.linear.y == twist2.linear.y\n",
    "        LZ = twist1.linear.z == twist2.linear.z\n",
    "        AX = twist1.angular.x == twist2.angular.x\n",
    "        AY = twist1.angular.y == twist2.angular.y\n",
    "        AZ = twist1.angular.z == twist2.angular.z\n",
    "        equal = LX and LY and LZ and AX and AY and AZ\n",
    "        if not equal:\n",
    "            rospy.logwarn(\"The current Twist is not the same as the one sent, Resending\")\n",
    "        return equal\n",
    "    \n",
    "    def move_robt(self, twist_object):\n",
    "        #we make this to avoid Topic loss, specially at the start\n",
    "        current_equal_to_new =  False\n",
    "        while (not(curent_equal_to_new) and not (self.shutdown_detected)):\n",
    "            self.cmd_vel_pub.publish(twist_object)\n",
    "            self._cmdvel_pub_rate.sleep()\n",
    "            current_equal_to_new =  self.compare_twist_commands(twist1=self.last_cmdvel_command, \n",
    "                                                               twist2=twist_object)\n",
    "            \n",
    "    def clean_class(self):\n",
    "        #stop Robot\n",
    "        twist_object = Twist()\n",
    "        twist_object.angular.z = 0.0\n",
    "        self.move_robot(twist_object)\n",
    "        self.shutdown_detected =  True\n",
    "    \n",
    "def main():\n",
    "    rospy.init_node('move_robot_node', anonymous=True)\n",
    "    \n",
    "    movekobuki_object = MoveKobuki()\n",
    "    twist_object =  Twist()\n",
    "    #Make it start turning\n",
    "    twist_object.angular.z = 0.5\n",
    "    \n",
    "    rate = rospy.Rate(5)\n",
    "    \n",
    "    ctrl_c = False\n",
    "    def shutdown():\n",
    "        #works better than the rospy.is_shut_down()\n",
    "        movekobuki_object.clean_class()\n",
    "        rospy.loginfo(\"shutdown time!\")\n",
    "        ctrl_c = True\n",
    "    \n",
    "    rospy.on_shutdown(shutdownhook)\n",
    "    \n",
    "    whiel not ctrl_c:\n",
    "        movekobuki_object.move_robot(twist_object)\n",
    "        rate.sleep()\n",
    "        \n",
    "if __main__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    END move_robot.py  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have an example of how all this code would be put together:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    follow_line_step_hsv.py \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "import cv2\n",
    "import numpy as np\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "from geometry_msgs.msg import Twist\n",
    "from sensor_msgs.msg import Image\n",
    "from move_robot import MoveKobuki\n",
    "\n",
    "class LineFollower(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.bridge_object = CvBridge()\n",
    "        self.image_sub = rospy.Subscriber('/camera/rgb/image_raw', Image, self.camera_callback)\n",
    "        self.movekobuki_object = MoveKobuki()\n",
    "        \n",
    "    def camera_callback(self, data):\n",
    "        \n",
    "        try:\n",
    "            #We celect rgb8 because its the OpenCV encoding by default\n",
    "            cv_image = self.bridge_object.image_to_cv2(data, desired_encoding=\"bgr8\")\n",
    "        except CvBridgeError as e:\n",
    "            print(e)\n",
    "            \n",
    "        # We get image dimendions and crop the parts of the image we dont need\n",
    "        # Bare in mind that because its image matrix first value is start and second value is down limit.\n",
    "        # Select the limits so that  it gets the line not too close, not to far and the minimum portion possible\n",
    "        # To make process faster\n",
    "        height, width, channels = cv_image.shape\n",
    "        \n",
    "        #descentre = 160\n",
    "        #row_to_watch = 20\n",
    "        \n",
    "        #descentre = 0\n",
    "        #row_to_watch = 20\n",
    "        \n",
    "        descentre = 0\n",
    "        row_to_watch = 200\n",
    "        \n",
    "        crop_img = cv_image[(height)/2 + descentre:(height)/2 + (descentre + rows_to_watch)][1:width]\n",
    "        \n",
    "        # Convert from RGB to HSV\n",
    "        hsv = cv2.cvtColor(crop_img, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # Define the Yellow Colour in HSV\n",
    "        #RGB\n",
    "        #[[[222, 255, 0]]]\n",
    "        #BGR\n",
    "        #[[[0, 255, 222]]]\n",
    "        \"\"\"\n",
    "        To know which color to track in HSV, Put in BGR. Use ColorZilla to get the color registered by the camera\n",
    "        >>> yellow = np.unit8[[[B, G,R]]]\n",
    "        >>> hsv_yellow = cv2.cvtColor(yellow, cv2.COLOR_BGR2HSV)\n",
    "        >>> print(hsv_yellow)\n",
    "        [[[34 255 255]]]        \n",
    "        \"\"\"\n",
    "        lower_yellow = np.array([20, 100, 100])\n",
    "        upper_yellow = np.array([50, 255, 255])\n",
    "        \n",
    "        lower_yellow = np.array([0, 55, 55])\n",
    "        upper_yellow = np.array([255, 255, 255])\n",
    "        \n",
    "        lower_yellow = np.array([33, 254, 254])\n",
    "        upper_yellow = np.array([36, 255, 255])\n",
    "        \n",
    "        #Blue Star, RGB = [3, 70, 255] --> HSV[111, 252, 225]\n",
    "        # Create a Filter for Blue star\n",
    "        lower_yellow = np.array([90, 249, 200])\n",
    "        upper_yellow = np.array([131, 255, 255])\n",
    "        \n",
    "        #Threshold the HSV image to get only yellow colors\n",
    "        mask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "        \n",
    "        # Calculate centroid of the blob of binary image using ImageMoments\n",
    "        m = cv2.moments(mask, False)\n",
    "        try:\n",
    "            cx,cy = m['m10']/m['m00'], m['m01']/m['m00']\n",
    "        except ZeroDivisionError:\n",
    "            cx, cy = height/2, width/2\n",
    "            \n",
    "        #Bitwise-And mask and original image\n",
    "        res =  cv2.bitwise_and(crop_img, crop_img, mask = mask)\n",
    "        \n",
    "        # Draw the centroid in the resulted image\n",
    "        # cv2.circle(img, center, radius, color[. thickness[, lineType[, shift]]])\n",
    "        cv2.circle(res, (int(cx), int(cy)), 10, (0,0,255), -1)\n",
    "        \n",
    "        cv2.imshow(\"Original\", cv_image)\n",
    "        cv2.imshow(\"HSV\", hsv)\n",
    "        cv2.imshow(\"MASK\", mask)\n",
    "        cv2.imshow(\"RES\", res)\n",
    "        \n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "        error_x = cx - width / 2;\n",
    "        twist_object = Twist()\n",
    "        twist_object.linear.x = 0.2;\n",
    "        twist_object.angular.z = -error_x / 100;\n",
    "        rospy.loginfo(\"ANGULAR VALUE SENT===>\" + str(twist_object.angular.z))\n",
    "        # Make it start turning\n",
    "        self.movekobuki_object.move_robot(twist_object)\n",
    "        \n",
    "    def clean_up(self):\n",
    "        self.movekobuki_object.clean_class()\n",
    "        cv2.destoryAllWindows()\n",
    "        \n",
    "def main():\n",
    "    rospy.init_node('line_following_node', anonymous=True)\n",
    "        \n",
    "    line_follower_object = LineFollower()\n",
    "        \n",
    "    rate = rospy.Rate(5)\n",
    "    ctrl_c = False\n",
    "    def shutdownhook():\n",
    "        #works better than the rospy.is_shut_down()\n",
    "        line_follower_object.clean_up()\n",
    "        rospy.loginfo(\"shutdown time!\")\n",
    "        ctrl_c =  True\n",
    "            \n",
    "    rospy.on_shutdown(shutdownhook)\n",
    "        \n",
    "    while not ctrl_c:\n",
    "        rate.sleep()\n",
    "            \n",
    "if __name__=='__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    END  follow_line_step_hsv.py \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    Exercise U2-2 \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a python script inside your package with the provided code. Execute it and see how it performs.\n",
    "<br>\n",
    "Try some improvements:\n",
    "* Lower the speed of the robot to see if if works better. Chage the linear and the angular speed.\n",
    "* Change the behaviour of the robot, maybe crate a recovery behaviour for when it looses the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    END Exercise U2-2 \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    Exercise U2-3 \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the colour to be tracked. Try to track back three different star colour. Use the colout picker for getting the exact RGB colour for:\n",
    "* RedStar\n",
    "* GreenStar\n",
    "* BlueStar\n",
    "\n",
    "<br>\n",
    "Here you have an example of how it should look with the blue star."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C11.png\" width=\"380\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also try changing the values of the upper and lower bounds to the following options and see results:\n",
    "* Loose colour detection: lower_yellow = np.arrary([0, 50, 50]), upper_yellow = np.array([255, 255, 255])\n",
    "* STRICT colour detection: lower_yellow = np.arrary([33, 254, 254]), upper_yellow = np.array([36, 255, 255])\n",
    "\n",
    "<br>\n",
    "Here you have an example of what you should see when changing the lower and upper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C12.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that in the loose version all the colour green and yellow are detected. While in the strict, you can see that even in the yellow line, there is a part that is cut of because its slightly different for the camera sensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    END Exercise U2-3 \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    Exercise U2-4\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try executing the code with different values in the **descentre** and the rows to watch and see how the follow line performs.<br> Try the following values:\n",
    "* descrentre = 0, rows_to_watch = 20. This case is that you control the center of the image and a very small piece.\n",
    "* descentre = 0, rows_to_watch = 200. This is the case in which you are controlling nearly all the lower part of the image.\n",
    "* descentre =  200, rows_to_watch=20. This case you are controlling a small fraction of the lower part of the image only.\n",
    "\n",
    "<br>\n",
    "Test these values and see which one is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see images like these:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C13.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C14.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C15.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    END Exercise U2-4 \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this first experience you might have seen that there are two main problems:\n",
    "1. When the robot follows a single line, it works, but when various paths are avaliable it just turns crazy until only one is in sight. This is because you do not have the info of multiple blobs detection to then papply a policy to select one direction.\n",
    "2. When the centroid is detected in the far end of the image the robot goes wild and turns too much and oscilates. This is due to the Proportional control used. This could be fixed using a complete PID control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will address these two issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Step: Follow Multiple Centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is the same code as before (*follow_line_step_hsv.py*) except for the fact that it tracks multiple blobs, letting you choose the path to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, _= cv2.findContout(mask, cv2.CHAIN_APPROX_TC89_L1)\n",
    "rospy.loginfo(\"Number of centroids==>\"+str(len(contours)))\n",
    "centres = []\n",
    "for i in range(len(contours[i]))\n",
    "    moments = cv2.moments(contours[i])\n",
    "    try:\n",
    "        centres.append((int(moments['m10']/moments['m00']), int(moments['m01']/moments['m00'])))\n",
    "        cv2.cricle(res, centres[-1], 10, (0, 255, 0), -1)\n",
    "        \n",
    "    except ZeroDividionError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the \"findContours\" functions to extract all the contours and then calculate the moments of each one. <br> If it the m00 is null, it considers that the centeres are useless and doesnt paint them. <br> It then paints a Green circle in each contour center. This gives the following result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C16.png\" width=\"380\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have the full code for refference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    follow_line_step_multiple.py \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "import cv2\n",
    "import numpy as np\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "from geometry_msgs.msg import Twist\n",
    "from sensor_msgs.msg import Image\n",
    "from move_robot import MoveKobuki\n",
    "\n",
    "class LineFollower(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.bridge_object = CvBridge()\n",
    "        self.image_sub = rospy.Subscriber('/camera/rgb/image_raw', Image, self.camera_callback)\n",
    "        self.movekobuki_object = MoveKobuki()\n",
    "        \n",
    "    def camera_callback(self, data):\n",
    "        \n",
    "        try:\n",
    "            #We celect rgb8 because its the OpenCV encoding by default\n",
    "            cv_image = self.bridge_object.image_to_cv2(data, desired_encoding=\"bgr8\")\n",
    "        except CvBridgeError as e:\n",
    "            print(e)\n",
    "            \n",
    "        # We get image dimendions and crop the parts of the image we dont need\n",
    "        # Bare in mind that because its image matrix first value is start and second value is down limit.\n",
    "        # Select the limits so that  it gets the line not too close, not to far and the minimum portion possible\n",
    "        # To make process faster\n",
    "        height, width, channels = cv_image.shape\n",
    "        \n",
    "        #descentre = 160\n",
    "        #row_to_watch = 20\n",
    "        \n",
    "        #descentre = 0\n",
    "        #row_to_watch = 20\n",
    "        \n",
    "        descentre = 0\n",
    "        row_to_watch = 200\n",
    "        \n",
    "        crop_img = cv_image[(height)/2 + descentre:(height)/2 + (descentre + rows_to_watch)][1:width]\n",
    "        \n",
    "        # Convert from RGB to HSV\n",
    "        hsv = cv2.cvtColor(crop_img, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # Define the Yellow Colour in HSV\n",
    "        #RGB\n",
    "        #[[[222, 255, 0]]]\n",
    "        #BGR\n",
    "        #[[[0, 255, 222]]]\n",
    "        \"\"\"\n",
    "        To know which color to track in HSV, Put in BGR. Use ColorZilla to get the color registered by the camera\n",
    "        >>> yellow = np.unit8[[[B, G,R]]]\n",
    "        >>> hsv_yellow = cv2.cvtColor(yellow, cv2.COLOR_BGR2HSV)\n",
    "        >>> print(hsv_yellow)\n",
    "        [[[34 255 255]]]        \n",
    "        \"\"\"\n",
    "        lower_yellow = np.array([20, 100, 100])\n",
    "        upper_yellow = np.array([50, 255, 255])\n",
    "        \n",
    "        lower_yellow = np.array([0, 55, 55])\n",
    "        upper_yellow = np.array([255, 255, 255])\n",
    "        \n",
    "        lower_yellow = np.array([33, 254, 254])\n",
    "        upper_yellow = np.array([36, 255, 255])\n",
    "        \n",
    "        #Blue Star, RGB = [3, 70, 255] --> HSV[111, 252, 225]\n",
    "        # Create a Filter for Blue star\n",
    "        lower_yellow = np.array([90, 249, 200])\n",
    "        upper_yellow = np.array([131, 255, 255])\n",
    "        \n",
    "        #Threshold the HSV image to get only yellow colors\n",
    "        mask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "        \n",
    "        # Calculate centroid of the blob of binary image using ImageMoments\n",
    "        m = cv2.moments(mask, False)\n",
    "        try:\n",
    "            cx,cy = m['m10']/m['m00'], m['m01']/m['m00']\n",
    "        except ZeroDivisionError:\n",
    "            cx, cy = height/2, width/2\n",
    "            \n",
    "        #Bitwise-And mask and original image\n",
    "        res =  cv2.bitwise_and(crop_img, crop_img, mask = mask)\n",
    "        \n",
    "        contours,_= cv2.findColours(mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_TC89_L1)\n",
    "        rospy.loginfo(\"Number of centroids==>\"+str(len(contours)))\n",
    "        centres = []\n",
    "        for i in range(len(contours)):\n",
    "            moments = cv2.moments(contours[i])\n",
    "            try:\n",
    "                centres.append((int(moments['m10']/moments['m00']), int(moments['m01']/moments['m00'])))\n",
    "                cv2.circle(res, centres[-1], 10, (0, 255, 0), -1)\n",
    "            except ZeroDivisionError:\n",
    "                pass\n",
    "            \n",
    "        rospy.loginfo(str(centres))\n",
    "                               \n",
    "        \n",
    "        # Draw the centroid in the resulted image\n",
    "        # cv2.circle(img, center, radius, color[. thickness[, lineType[, shift]]])\n",
    "        cv2.circle(res, (int(cx), int(cy)), 10, (0,0,255), -1)\n",
    "        \n",
    "        cv2.imshow(\"Original\", cv_image)\n",
    "        #cv2.imshow(\"HSV\", hsv)\n",
    "        #cv2.imshow(\"MASK\", mask)\n",
    "        cv2.imshow(\"RES\", res)\n",
    "        \n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "        error_x = cx - width / 2;\n",
    "        twist_object = Twist()\n",
    "        twist_object.linear.x = 0.2;\n",
    "        twist_object.angular.z = -error_x / 100;\n",
    "        rospy.loginfo(\"ANGULAR VALUE SENT===>\" + str(twist_object.angular.z))\n",
    "        # Make it start turning\n",
    "        self.movekobuki_object.move_robot(twist_object)\n",
    "        \n",
    "    def clean_up(self):\n",
    "        self.movekobuki_object.clean_class()\n",
    "        cv2.destoryAllWindows()\n",
    "        \n",
    "def main():\n",
    "    rospy.init_node('line_following_node', anonymous=True)\n",
    "        \n",
    "    line_follower_object = LineFollower()\n",
    "        \n",
    "    rate = rospy.Rate(5)\n",
    "    ctrl_c = False\n",
    "    def shutdownhook():\n",
    "        #works better than the rospy.is_shut_down()\n",
    "        line_follower_object.clean_up()\n",
    "        rospy.loginfo(\"shutdown time!\")\n",
    "        ctrl_c =  True\n",
    "            \n",
    "    rospy.on_shutdown(shutdownhook)\n",
    "        \n",
    "    while not ctrl_c:\n",
    "        rate.sleep()\n",
    "            \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    End follow_line_step_multiple.py \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    Exercise U2-5\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new python with this multiple contours system and try the following\n",
    "* Make the robot when faced with a dilema, select the right center to follow so that it stays in the cicular loop path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "End    Exercise U2-5\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "import cv2\n",
    "import numpy as np\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "from geometry_msgs.msg import Twist\n",
    "from sensor_msgs.msg import Image\n",
    "from move_robot import MoveKobuki\n",
    "\n",
    "class LineFollower(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.bridge_object = CvBridge()\n",
    "        self.image_sub = rospy.Subscriber('/camera/rgb/image_raw', Image, self.camera_callback)\n",
    "        self.movekobuki_object = MoveKobuki()\n",
    "        \n",
    "    def camera_callback(self, data):\n",
    "        \n",
    "        try:\n",
    "            #We celect rgb8 because its the OpenCV encoding by default\n",
    "            cv_image = self.bridge_object.image_to_cv2(data, desired_encoding=\"bgr8\")\n",
    "        except CvBridgeError as e:\n",
    "            print(e)\n",
    "            \n",
    "        # We get image dimendions and crop the parts of the image we dont need\n",
    "        # Bare in mind that because its image matrix first value is start and second value is down limit.\n",
    "        # Select the limits so that  it gets the line not too close, not to far and the minimum portion possible\n",
    "        # To make process faster\n",
    "        height, width, channels = cv_image.shape\n",
    "        \n",
    "        #descentre = 160\n",
    "        #row_to_watch = 20\n",
    "        \n",
    "        #descentre = 0\n",
    "        #row_to_watch = 20\n",
    "        \n",
    "        descentre = 0\n",
    "        row_to_watch = 200\n",
    "        \n",
    "        crop_img = cv_image[(height)/2 + descentre:(height)/2 + (descentre + rows_to_watch)][1:width]\n",
    "        \n",
    "        # Convert from RGB to HSV\n",
    "        hsv = cv2.cvtColor(crop_img, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # Define the Yellow Colour in HSV\n",
    "        #RGB\n",
    "        #[[[222, 255, 0]]]\n",
    "        #BGR\n",
    "        #[[[0, 255, 222]]]\n",
    "        \"\"\"\n",
    "        To know which color to track in HSV, Put in BGR. Use ColorZilla to get the color registered by the camera\n",
    "        >>> yellow = np.unit8[[[B, G,R]]]\n",
    "        >>> hsv_yellow = cv2.cvtColor(yellow, cv2.COLOR_BGR2HSV)\n",
    "        >>> print(hsv_yellow)\n",
    "        [[[34 255 255]]]        \n",
    "        \"\"\"\n",
    "        lower_yellow = np.array([20, 100, 100])\n",
    "        upper_yellow = np.array([50, 255, 255])\n",
    "        \n",
    "        lower_yellow = np.array([0, 55, 55])\n",
    "        upper_yellow = np.array([255, 255, 255])\n",
    "        \n",
    "        lower_yellow = np.array([33, 254, 254])\n",
    "        upper_yellow = np.array([36, 255, 255])\n",
    "        \n",
    "        #Blue Star, RGB = [3, 70, 255] --> HSV[111, 252, 225]\n",
    "        # Create a Filter for Blue star\n",
    "        lower_yellow = np.array([90, 249, 200])\n",
    "        upper_yellow = np.array([131, 255, 255])\n",
    "        \n",
    "        #Threshold the HSV image to get only yellow colors\n",
    "        mask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "        \n",
    "        # Calculate centroid of the blob of binary image using ImageMoments\n",
    "        m = cv2.moments(mask, False)\n",
    "        try:\n",
    "            cx,cy = m['m10']/m['m00'], m['m01']/m['m00']\n",
    "        except ZeroDivisionError:\n",
    "            cx, cy = height/2, width/2\n",
    "            \n",
    "        #Bitwise-And mask and original image\n",
    "        res =  cv2.bitwise_and(crop_img, crop_img, mask = mask)\n",
    "        \n",
    "        contours,_= cv2.findColours(mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_TC89_L1)\n",
    "        rospy.loginfo(\"Number of centroids==>\"+str(len(contours)))\n",
    "        centres = []\n",
    "        for i in range(len(contours)):\n",
    "            moments = cv2.moments(contours[i])\n",
    "            try:\n",
    "                centres.append((int(moments['m10']/moments['m00']), int(moments['m01']/moments['m00'])))\n",
    "                cv2.circle(res, centres[-1], 10, (0, 255, 0), -1)\n",
    "            except ZeroDivisionError:\n",
    "                pass\n",
    "            \n",
    "        rospy.loginfo(str(centres))\n",
    "        #Select the right centroid\n",
    "        #[(542, 39), (136, 46)], (x, y)\n",
    "        most_right_centroid_index = 0\n",
    "        index = 0\n",
    "        max_x_value = 0\n",
    "        for candidate in centres:\n",
    "            #Retrieve the cx value\n",
    "            cx = candidate[0]\n",
    "            #Get the Cx more to the right\n",
    "            if cx >= max_xvalue:\n",
    "                max_xvalue = cx\n",
    "                most_right_centroid_index = index\n",
    "            index +=1\n",
    "            \n",
    "        try:\n",
    "            cx = centres[most_right_centroid_index][0]\n",
    "            cy = centres[most_right_centroid_index][1]\n",
    "            rospy.logwarn(\"Winner ==\"+str(cx)+\",\"+str(cy)+\"\")\n",
    "        except:\n",
    "            cx , cy = height/2, width/2\n",
    "                               \n",
    "        \n",
    "        # Draw the centroid in the resulted image\n",
    "        # cv2.circle(img, center, radius, color[. thickness[, lineType[, shift]]])\n",
    "        cv2.circle(res, (int(cx), int(cy)), 10, (0,0,255), -1)\n",
    "        \n",
    "        cv2.imshow(\"Original\", cv_image)\n",
    "        #cv2.imshow(\"HSV\", hsv)\n",
    "        #cv2.imshow(\"MASK\", mask)\n",
    "        cv2.imshow(\"RES\", res)\n",
    "        \n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "        error_x = cx - width / 2;\n",
    "        twist_object = Twist()\n",
    "        twist_object.linear.x = 0.2;\n",
    "        twist_object.angular.z = -error_x / 100;\n",
    "        rospy.loginfo(\"ANGULAR VALUE SENT===>\" + str(twist_object.angular.z))\n",
    "        # Make it start turning\n",
    "        self.movekobuki_object.move_robot(twist_object)\n",
    "        \n",
    "    def clean_up(self):\n",
    "        self.movekobuki_object.clean_class()\n",
    "        cv2.destoryAllWindows()\n",
    "        \n",
    "def main():\n",
    "    rospy.init_node('line_following_node', anonymous=True)\n",
    "        \n",
    "    line_follower_object = LineFollower()\n",
    "        \n",
    "    rate = rospy.Rate(5)\n",
    "    ctrl_c = False\n",
    "    def shutdownhook():\n",
    "        #works better than the rospy.is_shut_down()\n",
    "        line_follower_object.clean_up()\n",
    "        rospy.loginfo(\"shutdown time!\")\n",
    "        ctrl_c =  True\n",
    "            \n",
    "    rospy.on_shutdown(shutdownhook)\n",
    "        \n",
    "    while not ctrl_c:\n",
    "        rate.sleep()\n",
    "            \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see its the excat same code, just that it uses only the multiple centroids and it selects the one that has the higher value of X, and therefore going always to the right. You can see also that a red circle is drawn in the centroid selected to be able to see it visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PID controller with perception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlling a robot means moving the robot based on the info provided by some sensor data. Everything you work with perception for robots you will have this coupling. So you better start learning a little bit of control so you can take the most of your robot perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of controlling a bit better the movement of the robot and make it smoother is to apply a PID controller to the control values. Luckily for us there is a [**PID ROS package**](http://wiki.ros.org/pid) that makes using PIDs much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you have to get used to this new tool, so here is an example for testing how the PID Package works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    pid_test.launch\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first launch launches all the need elements for PID ROS to work and also you own PID test script \"pid_control.py\". <br> It launches the following:\n",
    "* pkg=\"pid\" type=\"control\". This starts the PID control system with the values given. PID values, and also some upper and lower limits where the signal value can't exceed. It also sets the windup limit and a range in the loop frequency. This loop frequency depends on how precise you want the control to be and depends on how fast the system changes.\n",
    "* pid_control.py: The one that published the values desired, and the state of the syste. It makes the function of the real system or simulated system.\n",
    "* rqt_plot: Will plot the value of the PID, so the */setpoint*, the*/state* and the */control_effort* in the Graphical Tools space.\n",
    "* rqt_reconfigure: This will allow you to change in execution time the PID values to see the effects immediatelly in the Graphical Tools space."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<launch>\n",
    "  <node name=\"follow_line_pid\" pkg=\"pid\" type=\"contoller\" >\n",
    "    <param name=\"Kp\" value=\"1.0\" />\n",
    "    <param name=\"Ki\" value=\"0.0\" />\n",
    "    <param name=\"Kd\" value=\"0.1\" />\n",
    "    <param name=\"upper_limit\" value=\"2.0\" />\n",
    "    <param name=\"lower_limit\" value=\"-2.0\" />\n",
    "    <param name=\"window_limit\" value=\"2.0\" />\n",
    "    <param name=\"max_loop_frequency\" value=\"10.0\" />\n",
    "    <param name=\"min_loop_frequency\" value=\"5.0\" />\n",
    "   </node>\n",
    "   \n",
    "   <node name=\"pid_node\" pkg=\"my_following_line_package\" type=\"pid_control.py\" output=\"screen\"/>\n",
    "   \n",
    "   <node name=\"rqt_plot\" pkg=\"rqt_plot\" type=\"rqt_plot\" args=\"/control_effort/data/state/data/setpoint/data\" />\n",
    "   \n",
    "   <node name=\"rqt_reconfigure\" pkg=\"rqt_reconfigure\" type=\"rqt_reconfigure\" />\n",
    "  \n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    End pid_test.launch\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    pid_control.py\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python script works in this way:\n",
    "* You set a desired value to reach, publishing in the \"/setpoint\" topic. In this case is \"0.0\"\n",
    "* You also publish the \"Real\" state of your system in \"/state\" topic, in this case its a sine wave that is not affected in reality by the control. But this will make the PID system work to try and move the sine wave to the desired value 0.0\n",
    "* You susbcribe to the \"/control_effort\", to see what values the controller is issuing to try and make the state value move to the setpoint, 0.0 in this case. It should be more or less an inverted sine wave to the state value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "from std_msgs.msg import Float64\n",
    "from random import randint\n",
    "from math import sin\n",
    "\n",
    "class PID(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self._setpoint_hub = rospy.Publisher(\"/setpoint\", Float64, queue_size=1)\n",
    "        self._state_pub = rospy.Publisher(\"/state\", Float64, queue_size=1)\n",
    "        self._control_effort_sub = rospy.Subscriber('/control_effort', Float64. self.control_effort_callback)\n",
    "        self._control_effort_value = Float64()\n",
    "        \n",
    "    def control_effort_callback(self, data):\n",
    "        self._control_effort_value = data.data\n",
    "        \n",
    "    def setpoint_update(self, value):\n",
    "        value_object = Float64()\n",
    "        value_object.data = value\n",
    "        self._setpoint_hub.publish(value_object)\n",
    "        \n",
    "    def state_update(self, value):\n",
    "        value_object = Float64()\n",
    "        value_object.data = value\n",
    "        self._state_hub.publish(value_object)\n",
    "        \n",
    "    def get_control_effort(self):\n",
    "        return self._control_effort_value.data\n",
    "    \n",
    "def sinus_test():\n",
    "    rospy.init_node('sinus_pid_node', anonymous=True)\n",
    "    \n",
    "    pid_object = PID()\n",
    "    \n",
    "    rate = rospy.Rate(10.0)\n",
    "    ctrl_c = False\n",
    "    def shutdownhook():\n",
    "        rospy.loginfo(\"shutdown time!\")\n",
    "        ctrl_c =  True\n",
    "        \n",
    "    rospy.on_shutdown(shutdownhook)\n",
    "    setPoint_value = 0.0\n",
    "    pid_object.setpoint_update(value=setPoint_value)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while not ctrl_c:\n",
    "        state_value = sin(i)\n",
    "        pid_object.state_update(value=state_value)\n",
    "        effort_value = pid_object.get_control_effort()\n",
    "        #print(\"state_value ==>\"+str(state_value))\n",
    "        #print(\"effort_value ==>\"+str(effort_value))\n",
    "        rate.sleep()\n",
    "        i +=0.1\n",
    "        \n",
    "def step_test():\n",
    "    rospy.init_node('step_pid_node', anonymous=True)\n",
    "    \n",
    "    pid_object = PID()\n",
    "    \n",
    "    rate = rospy.Rate(10.0)\n",
    "    ctrl_c =  False\n",
    "    def shutdownhook():\n",
    "        rospy.loginfo(\"shutdown time!\")\n",
    "        ctrl_c =  True\n",
    "        \n",
    "    rospy.on_shutdown(shutdownhook)\n",
    "    setPoint_value = 0.0\n",
    "    pid_object.setpoint_update(value=setPoint_value)\n",
    "    \n",
    "    i = 0\n",
    "    state_value = 1.0\n",
    "    \n",
    "    while not ctrl_c:\n",
    "\n",
    "        pid_object.state_update(value=state_value)\n",
    "        effort_value = pid_object.get_control_effort()\n",
    "        #print(\"state_value ==>\"+str(state_value))\n",
    "        #print(\"effort_value ==>\"+str(effort_value))\n",
    "        rate.sleep()\n",
    "        i +=0.1\n",
    "        if i > 30:\n",
    "            state_value *= -1\n",
    "            i = 0\n",
    "            \n",
    "if __main__=='__main__':\n",
    "    #step_test()\n",
    "    sinus_test()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see sth similar to this in the Graphical Tools Tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C18.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you are seeing is the step_test, where a step value of 1 and -1 is alternated. The Red line is the */set_point*. Is the value you set. And the Blue line is the effort, the signal issued by the PID to counteract that signal. Because this is dummy test, and there is no real physical system, the effort has no effect whatsoever. This is why the effort stays at an inverse value of the signal. In Real system it would affect the state and therefore the effort would diminish until the \"set_point\" and the \"state\" had the same value.\n",
    "<br>\n",
    "Also note that because you have set a upper_limit of 2 and lower_limit of -2, the effort values can't exceed those values. This can be seen when the Kd is increased to 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you dont know what is going on here, please have a look at this pratical ***PID_Guide***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    End pid_control.py\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    Exercise U2-6 \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Execute the code given and see the responce depending on the PID values given.\n",
    "* See how increasing the Kd the values go faster but have higher peak value.\n",
    "* See how setting the upper_limit and lower_limit affect also the behaviour of the effort value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    END Exercise U2-6  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    Exercise U2-7 \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this knowledge inot the robot system.\n",
    "* Create a launch file for your new follow_line python script that uses the PID.\n",
    "* This launch will not only launch your follow_line_script but also the PID package for the control.\n",
    "* If you want you also can plot the values with rqt_plot and start the reconfigure for tweeking the PID values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "* Make the Robot follow smoothly the lines, no big oscilations but fast response. It has to be precise in its movements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    END Exercise U2-7  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    pid_movement.launch\n",
    "</p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<launch>\n",
    "  <!-- Limits are base on the camera width of 640 -->\n",
    "  <node name=\"follow_line_pid\" pkg=\"pid\" type=\"contoller\" >\n",
    "    <param name=\"Kp\" value=\"0.8\" />\n",
    "    <param name=\"Ki\" value=\"0.0\" />\n",
    "    <param name=\"Kd\" value=\"0.05\" />\n",
    "    <param name=\"upper_limit\" value=\"640\" />\n",
    "    <param name=\"lower_limit\" value=\"-640\" />\n",
    "    <param name=\"window_limit\" value=\"640\" />\n",
    "    <param name=\"max_loop_frequency\" value=\"10.0\" />\n",
    "    <param name=\"min_loop_frequency\" value=\"10.0\" />\n",
    "   </node>\n",
    "   \n",
    "   \n",
    "   <node name=\"rqt_plot\" pkg=\"rqt_plot\" type=\"rqt_plot\" args=\"/control_effort/data/state/data/setpoint/data\" />\n",
    "   \n",
    "   <node name=\"rqt_reconfigure\" pkg=\"rqt_reconfigure\" type=\"rqt_reconfigure\" />\n",
    "  \n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    END pid_movement.launch\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the main launch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    follow_line_with_pid.launch \n",
    "</p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<launch>\n",
    "    <include file=\"$(find my_following_line_package)/launch/pid_movement.launch\" />\n",
    "    <node name=\"line_following_node\" pkg=\"my_following_line_package\" type=\"follow_line_step_pid.py\" output=\"screen\" />\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is a way of moving the robot with PID. In this case no multiple centrods is searches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "follow_line_step_pid.py\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "import cv2\n",
    "import numpy as np\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "from geometry_msgs.msg import Twist\n",
    "from sensor_msgs.msg import Image\n",
    "from move_robot import MoveKobuki\n",
    "from pid_control import PID\n",
    "\n",
    "class LineFollower(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        rospy.lagwarn(\"Init line Follower\")\n",
    "        self.bridge_object = CvBridge()\n",
    "        self.image_sub = rospy.Subscriber('/camera/rgb/image_raw', Image, self.camera_callback)\n",
    "        self.movekobuki_object = MoveKobuki()\n",
    "        self.pid_object = PID()\n",
    "        \n",
    "    def camera_callback(self, data):\n",
    "        \n",
    "        try:\n",
    "            #We celect rgb8 because its the OpenCV encoding by default\n",
    "            cv_image = self.bridge_object.image_to_cv2(data, desired_encoding=\"bgr8\")\n",
    "        except CvBridgeError as e:\n",
    "            print(e)\n",
    "            \n",
    "        # We get image dimendions and crop the parts of the image we dont need\n",
    "        # Bare in mind that because its image matrix first value is start and second value is down limit.\n",
    "        # Select the limits so that  it gets the line not too close, not to far and the minimum portion possible\n",
    "        # To make process faster\n",
    "        height, width, channels = cv_image.shape\n",
    "        \n",
    "        #descentre = 160\n",
    "        #row_to_watch = 20\n",
    "        \n",
    "        #descentre = 0\n",
    "        #row_to_watch = 20\n",
    "        \n",
    "        descentre = 0\n",
    "        row_to_watch = 200\n",
    "        \n",
    "        crop_img = cv_image[(height)/2 + descentre:(height)/2 + (descentre + rows_to_watch)][1:width]\n",
    "        \n",
    "        # Convert from RGB to HSV\n",
    "        hsv = cv2.cvtColor(crop_img, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # Define the Yellow Colour in HSV\n",
    "        #RGB\n",
    "        #[[[222, 255, 0]]]\n",
    "        #BGR\n",
    "        #[[[0, 255, 222]]]\n",
    "        \"\"\"\n",
    "        To know which color to track in HSV, Put in BGR. Use ColorZilla to get the color registered by the camera\n",
    "        >>> yellow = np.unit8[[[B, G,R]]]\n",
    "        >>> hsv_yellow = cv2.cvtColor(yellow, cv2.COLOR_BGR2HSV)\n",
    "        >>> print(hsv_yellow)\n",
    "        [[[34 255 255]]]        \n",
    "        \"\"\"\n",
    "        lower_yellow = np.array([20, 100, 100])\n",
    "        upper_yellow = np.array([50, 255, 255])\n",
    "        \n",
    "        \n",
    "        #Threshold the HSV image to get only yellow colors\n",
    "        mask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "        \n",
    "        # Calculate centroid of the blob of binary image using ImageMoments\n",
    "        m = cv2.moments(mask, False)\n",
    "        try:\n",
    "            cx,cy = m['m10']/m['m00'], m['m01']/m['m00']\n",
    "        except ZeroDivisionError:\n",
    "            cx, cy = height/2, width/2\n",
    "            \n",
    "        #Bitwise-And mask and original image\n",
    "        res =  cv2.bitwise_and(crop_img, crop_img, mask = mask)\n",
    "                               \n",
    "        \n",
    "        # Draw the centroid in the resulted image\n",
    "        # cv2.circle(img, center, radius, color[. thickness[, lineType[, shift]]])\n",
    "        cv2.circle(res, (int(cx), int(cy)), 10, (0,0,255), -1)\n",
    "        \n",
    "        #cv2.imshow(\"Original\", cv_image)\n",
    "        #cv2.imshow(\"HSV\", hsv)\n",
    "        #cv2.imshow(\"MASK\", mask)\n",
    "        cv2.imshow(\"RES\", res)\n",
    "        \n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "        #Move the Robot, center it in the middle of the width 640 => 320:\n",
    "        setPoint_value = width / 2\n",
    "        self.pid_object.setpoint_update(value=setPoint_value)\n",
    "        \n",
    "        twist_object = Twist()\n",
    "        twist_object.linear.x = 0.2;\n",
    "        \n",
    "        #Make it start turning\n",
    "        self.pid_object.state_update(value=cx)\n",
    "        effort_value = self.pid_object.get_control_effort()\n",
    "        # We divide the effort to map it to the normal values for angular speed in the turtlebot\n",
    "        rospy.lagwarn(\"Set Value==\"+str(setPoint_value))\n",
    "        rospy.lagwarn(\"State Value==\"+str(cx))\n",
    "        rospy.lagwarn(\"Effort Value==\"+str(effort_value))\n",
    "        angular_effort_value = effort_value / 200.0\n",
    "         \n",
    "        twist_object.angular.z = angular_effort_value;\n",
    "        rospy.loginfo(\"Twist===>\" + str(twist_object.angular.z))\n",
    "        # Make it start turning\n",
    "        self.movekobuki_object.move_robot(twist_object)\n",
    "        \n",
    "    def clean_up(self):\n",
    "        self.movekobuki_object.clean_class()\n",
    "        cv2.destoryAllWindows()\n",
    "        \n",
    "def main():\n",
    "    rospy.init_node('line_following_node', anonymous=True)\n",
    "        \n",
    "    line_follower_object = LineFollower()\n",
    "        \n",
    "    rate = rospy.Rate(5)\n",
    "    ctrl_c = False\n",
    "    def shutdownhook():\n",
    "        #works better than the rospy.is_shut_down()\n",
    "        line_follower_object.clean_up()\n",
    "        rospy.loginfo(\"shutdown time!\")\n",
    "        ctrl_c =  True\n",
    "            \n",
    "    rospy.on_shutdown(shutdownhook)\n",
    "        \n",
    "    while not ctrl_c:\n",
    "        rate.sleep()\n",
    "            \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Move the Robot, center it in the middle of the width 640 => 320:\n",
    "setPoint_value = width / 2\n",
    "self.pid_object.setpoint_update(value=setPoint_value)\n",
    "        \n",
    "twist_object = Twist()\n",
    "twist_object.linear.x = 0.2;\n",
    "        \n",
    "#Make it start turning\n",
    "self.pid_object.state_update(value=cx)\n",
    "effort_value = self.pid_object.get_control_effort()\n",
    "# We divide the effort to map it to the normal values for angular speed in the turtlebot\n",
    "rospy.lagwarn(\"Set Value==\"+str(setPoint_value))\n",
    "rospy.lagwarn(\"State Value==\"+str(cx))\n",
    "rospy.lagwarn(\"Effort Value==\"+str(effort_value))\n",
    "angular_effort_value = effort_value / 200.0\n",
    "         \n",
    "twist_object.angular.z = angular_effort_value;\n",
    "rospy.loginfo(\"Twist===>\" + str(twist_object.angular.z))\n",
    "# Make it start turning\n",
    "self.movekobuki_object.move_robot(twist_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In escence you are setting the \"setPoint\", which is always to try to have the center of the line in the middle of the image.\n",
    "<br>\n",
    "Then the sate, which in this case is the centroid of the blob, is published. \n",
    "<br>\n",
    "And fianlly the effort given by the control PID running, is retried, converted to a sensible value then the twist is published."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    Exercise Extra U2-8\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a definitive script that can follow the correct path (loop path) with the PID control and the multiple centroids info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    End Exercise Extra U2-8\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    Exercise Extra U2-9\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new following script that when published on a topic called \"objective\" the colour to follow will:\n",
    "* If yellow will follow the loop path\n",
    "* If red will follow the loop path until it reaches to the red star path\n",
    "* If green will follow the loop path until it reaches to the green star path\n",
    "* If blue will follow the loop path until it reaches to the blue star path\n",
    "* If none, it will stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> \n",
    "    End Exercise Extra U2-9\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Congratulations!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**OpenCV**](https://docs.opencv.org/master/d9/df8/tutorial_root.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> \n",
    "    END line_follower_basics.py \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/C1.png\" width=\"380\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
