{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most usefull perception skills is being able to recognise objects. This allows creating robots that grasp objects and understand a bit better the world  around it.\n",
    "* Recognise flat surfaces: This skill allows to detect places where objects.\n",
    "* Recognise objects: once you know where to look you have to be able to recongnise different object in the scene and localise where the are from your robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"/base_controller/command\" through Twist commands. You also can use other systems for moving the torso up and down and the head to look around. But in this object recognition only the movement of the base is really necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Top Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in recognising objects is knowing where these objects are. For this you are going to use a port of the **tabletop_object_detector** package to be able to detect flats surfaces and represent that detection in RVIZ. (Object Recognition Kitchen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as usual the first step is to create your own object_recognition package:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute in WebShell**\n",
    "<br>\n",
    "roscd: cd ../src\n",
    "<br>\n",
    "catkin_create_pkg my_object_recognition_pkg rospy object_recognition_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have to add dependencies on each new package to need inside. \n",
    "<br>\n",
    "So you are going to use this launch file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> init_table_top.launch </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-3deeb299568a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3deeb299568a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <launch>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<launch>\n",
    "\n",
    "    <arg name=\"tabletop_ork_file\" value=\"$(find my_object_recognition_pkg)/conf/detection.table_fetch.ros.ork\"/>\n",
    "    \n",
    "    <node pkg=\"object_recognition_core\"\n",
    "    type=\"detection\"\n",
    "    name=\"tabletop_server_node\"\n",
    "    args=\"-c $(arg tabletop_ork_file)\"\n",
    "    output=\"screen\">\n",
    "    </node>\n",
    "    \n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> END init_table_top.launch  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, you are launching a binary called **detection** with a configuration file as argument. This file called **detection.tabletop_fetch.ros.ork** ist where all the input sensors and values for the table detection are set. Its a YAML file with a different extension .ork. So the first thing is create a *conf* directory inside your package. Then create this file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> detection.tabletop_fetch.ros.ork  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source1:\n",
    "    type: RosKinect\n",
    "    module: 'object_recognition_ros.io'\n",
    "    parameters:\n",
    "        rgb_frame_id: '/head_camera_rgb_optical_frame'\n",
    "        rgb_image_topic: '/head_camera/rgb/image_raw'\n",
    "        rgb_camera_info: '/head_camera/rgb/camera_info'\n",
    "        depth_image_topic: '/head_camera/depth_registered/image_raw'\n",
    "        depth_camera_info: '/head_camera/depth_registered/camera_info'\n",
    "        #\n",
    "        #crop_enabled: True\n",
    "        #x_min: -0.4\n",
    "        #x_max: 0.4\n",
    "        #y_min: -1.0\n",
    "        #y_max: 0.2\n",
    "        #z_min: 0.3\n",
    "        #z_max: 1.5\n",
    "        \n",
    "sink1:\n",
    "    type: TablePublisher\n",
    "    module: 'object_recognition_tabletop'\n",
    "        inputs: [source1]\n",
    "            \n",
    "pipeline1:\n",
    "    type: TabletopTableDetector\n",
    "    module: 'object_recognition_tabletop'\n",
    "    inputs: [source1]\n",
    "    outputs: [sink1]\n",
    "    parameters:\n",
    "        table_detector:\n",
    "            min_table_size: 4000\n",
    "            plane_threshold: 0.01\n",
    "        #cluster:\n",
    "        # table_z_filter_max: 0.35\n",
    "        # table_z_filter_min: 0.025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> END  detection.tabletop_fetch.ros.ork   </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all the parameters you have here, the only relevant most of the times are the:\n",
    "* rgb_frame_id: '/head_camera_rgb_optical_frame'\n",
    "* rgb_image_topic: '/head_camera/rgb/image_raw'\n",
    "* rgb_camera_info: '/head_camera/rgb/camera_info'\n",
    "* depth_image_topic: '/head_camera/depth_registered/image_raw'\n",
    "* depth_camera_info: '/head_camera/depth_registered/camera_info'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set the correct image topics as inputs so that the recognition can be made. Once you have it, just execute the launch file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute in WebShell**\n",
    "<br>\n",
    "roslaunch my_object_recognition_pkg init_table_top.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output you should get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/R1.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now open the RVIZ and add all the element you want to see (like the Camera element or PointCloud2 elements). To visualise the Table detection you will have to add **OrkTable** element. You select the topic where the table data is published, in this case **/table_array**. You can then check some options like \"bounding_box\" to have a bounding box around the detection or the \"top\" option to see what is being considered as the top of the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at all the data textracted from **/table_array** topic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/R2.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might see you can pinpoint the location of any surface detected or even filter the floor, because you know at waht hight each surface is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise U3-1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a script that makes the Fetch robot aproach to the table based on the data from the /table_array. This will allow you to look for a table and reach it for after searching for objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> Help Exercise U3-1   </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#:/usr/bin/env python\n",
    "\n",
    "import numpy \n",
    "import copy\n",
    "import actionlib\n",
    "import rospy\n",
    "import time\n",
    "from math import sin, cos\n",
    "from moveit_python import (MoveGroupInterface, \n",
    "                          PlanningSceneInterface,\n",
    "                          PickPlaceInterface)\n",
    "from moveit_python.geometry import rotate_pose_msg_by_euler_angles\n",
    "\n",
    "from control_msgs.msg import FollowJointTrajectoryAction, FollowJointTrajectoryGoal\n",
    "from control_msgs.msg import PointHeadAction, PointHeadGoal\n",
    "from geometry_msg.msg import Twist, Pose\n",
    "from move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\n",
    "from moveit_msgs.msg import JointTrajectory, JointTrajectoryPoint\n",
    "from nav_msgs.msg import Odometry\n",
    "\n",
    "#Move base using navigation stack\n",
    "class MoveBasePublisher(object):\n",
    "    \n",
    "    def init (self):\n",
    "        self.move_base_publisher = rospy. Publisher('/base_controller/command', Twist, queue_size=1)\n",
    "        self.odom_subs = rospy.Subscriber('/odom', Odometry, self.odom_callback)\n",
    "        self.actual_pose = Pose()\n",
    "        \n",
    "    def odom_callback(self, msg):\n",
    "        self.actual_pose = msg.pose.pose\n",
    "        \n",
    "    def move_to(self, twist_object):\n",
    "        self._move_base_publisher.publish(twist_object)\n",
    "        \n",
    "    def stop(self):\n",
    "        twist_object = Twist()\n",
    "        self.move_to(twist_object)\n",
    "        \n",
    "    def move_forwards_backwards(self, speed_ms, position_x, allow_error=0.1):\n",
    "        \"\"\"\n",
    "        distance_metres: positive is go forwards, negative ga backwards\n",
    "        \"\"\"\n",
    "        rate = rospy.Rate(5)\n",
    "        move_twist = Twist()\n",
    "        \n",
    "        start_x = self.actual_pose.position.x\n",
    "        direction_speed = numpy.sign(position_x - start.x)\n",
    "        move_twist.linear.x = direction_speed * speed_ms\n",
    "        \n",
    "        in_place = False\n",
    "        while not in_place:\n",
    "            self.move_to(move_twist)\n",
    "            x_actual = self.actual_pose.position.x\n",
    "            print (x_actual)\n",
    "            \n",
    "            if abs(position_x - x_actual) <= allowed_error:\n",
    "                print (\"Reached position\")\n",
    "                break\n",
    "                \n",
    "            rate.sleep()\n",
    "        self.stop()\n",
    "        \n",
    "# Move base using naviagtion stack\n",
    "class MoveBaseClient(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = actionlib.SimpleActionClient(\"move_base\", MoveBaseAction)\n",
    "        rospy.loginfo(\"Waiting for move_base...\")\n",
    "        self.client.wait_for_server()\n",
    "        \n",
    "    def goto(self, x, y, theta, frame=\"map\"):\n",
    "        move_goal = MoveBaseGoal()\n",
    "        move_goal.target_pose.pose.position.x = x\n",
    "        move_goal.target_pose.pose.position.y = y\n",
    "        move_goal.target_pose.pose.orientation.z = sin(theta/2.0)\n",
    "        move_goal.target_pose.pose.orientation.w = cos(theta/2.0)\n",
    "        move_goal.target_pose.header.frame_id = frame\n",
    "        move_goal.target_pose.header.stamp = rospy.Time.now()\n",
    "        \n",
    "        #TODO wait for things to work\n",
    "        self.client.send_goal(move_goal)\n",
    "        self.client.wait_for_result()\n",
    "        \n",
    "#Send a trajectory to controller\n",
    "class FollowTrajectoryClient(object):\n",
    "    \n",
    "    def __init__(self, name, joint_names):\n",
    "        self.client = actionlib.SimpleActionClient(\"%s/follow_joint_trajectory\" % name,\n",
    "                                                  FollowJointTrajectoryAction)\n",
    "        rospy.loginfo(\"Waiting for %s...\" % name)\n",
    "        self.client.wait_for_server()\n",
    "        self.joint_names = joint_names\n",
    "        \n",
    "    def move_to(self, positions, duration=5.0):\n",
    "        if len(self.joint_names) != len(positions):\n",
    "            print(\"Invalid trajecotry position\")\n",
    "            return False\n",
    "        trajectory = JointTrajectory()\n",
    "        trajectory.joint_names = self.joint_names\n",
    "        trajectory.points.append(JointTrajectoryPoint())\n",
    "        trajectory.points[0].position = positions\n",
    "        trajectory.points[0].velocities = [0.0 for _ in positions]\n",
    "        trajectory.points[0].accelerations = [0.0 for _ in positions]\n",
    "        trajectory.points[0].time_from_start = rospy.Duration(duration)\n",
    "        follow_goal = FollowJointTrajectoryGoal()\n",
    "        follow_goal.trajectory = trajectory\n",
    "        \n",
    "        self.client.send_goal(follow_goal)\n",
    "        self.client.wait_for_result()\n",
    "        \n",
    "# Point the head using controller\n",
    "class PointHeadClient(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = actionlib.SimpleActionClient(\"head_controller/point_head\", PointHeadAction)\n",
    "        rospy.loginfo(\"Waiting for head_controller...\")\n",
    "        self.client.wait_for_server()\n",
    "        \n",
    "    def look_at(self, x, y, z, frame, duration=1.0):\n",
    "        goal = PointHeadGoal()\n",
    "        goal.target.header.stamp = rospy.Time.now()\n",
    "        goal.target.header.frame_id = frame\n",
    "        goal.target.point.x = x \n",
    "        goal.target.point.y = y\n",
    "        goal.target.point.z = z\n",
    "        goal.min_duration = rospy.Duration(duration)\n",
    "        self.client.send_goal(goal)\n",
    "        self.client.wait_for_result()\n",
    "        \n",
    "# Tools for grasping\n",
    "class GraspingClient(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scene = PlanningSceneInterface(\"base_link\")\n",
    "        self.pickplace = PickPlaceInterface(\"arm\", \"gripper\", verbose=True)\n",
    "        self.move_group = MoveGroupInterface(\"arm\", \"base_link\")\n",
    "        \n",
    "    def getGraspableCube(self):\n",
    "        \n",
    "        #nothing detected\n",
    "        return None, None\n",
    "    \n",
    "    def getSupportSurface(self, name):\n",
    "        for surface in self.support_surfaces:\n",
    "            if surface.name == name:\n",
    "                return surface\n",
    "        return None\n",
    "    \n",
    "    def getPlaceLocation(self):\n",
    "        pass\n",
    "    \n",
    "    def pick(self, block, grasps):\n",
    "        success, pick_result = self.pickplace.pick_with_retry(block.name,\n",
    "                                                             grasps,\n",
    "                                                             support_name=block.support_surface,\n",
    "                                                             scene=self.scene)\n",
    "        self.pick_result = pick_result\n",
    "        return success\n",
    "    \n",
    "    def place(self, block, pose_stamped):\n",
    "        places= list()\n",
    "        l = PlaceLocation()\n",
    "        l.place_pose.pose = pose_stamped.pose\n",
    "        l.place_pose.header.frame_id = pose_stamped.header.framed_id\n",
    "        \n",
    "        # copy the posture, approach and retreat from the grasp used\n",
    "        l.pose_place_posture =  slef.pick_result.grasp.pre_grasp_posture\n",
    "        l.pose_place_approach =  slef.pick_result.grasp.pre_grasp_approach\n",
    "        l.pose_place_retreat =  slef.pick_result.grasp.post_grasp_retreat\n",
    "        # create another several places, rotate each by 90 degrees in yaw direction\n",
    "        l.place_pose.pose = rotate_pose_msg_by_euler_angles(l.place_pose.pose, 0, 0, 1.57)\n",
    "        places.append(copy.deepcopy(l))\n",
    "        l.place_pose.pose = rotate_pose_msg_by_euler_angles(l.place_pose.pose, 0, 0, 1.57)\n",
    "        places.append(copy.deepcopy(l))\n",
    "        l.place_pose.pose = rotate_pose_msg_by_euler_angles(l.place_pose.pose, 0, 0, 1.57)\n",
    "        places.append(copy.deepcopy(l))\n",
    "        \n",
    "        succes, palce_result = self.pickplace.place_with_retry(block.name, \n",
    "                                                              places,\n",
    "                                                              scene=self.scene)\n",
    "        return success\n",
    "    \n",
    "    def stuk(self):\n",
    "        joints = [\"shoulder_pan_joint\", \"shoulder_lift_joint\", \"upperarm_roll_joint\",\n",
    "                 \"elbow_flex_joint\", \"forearm_roll_joint\", \"wrist_flex_joint\", \"wrist_roll_joint\"]\n",
    "        pose = [1.32, 1.40, -0.2, 1.72, 0.0, 1.66, 0.0]\n",
    "        while not rospy.is_shutdown():\n",
    "            result = self.move_group.moveToJointPosition(joints, pose, 0.02)\n",
    "            if result.error_code.val == MoveItErrorCodes.SUCCESS:\n",
    "                return\n",
    "        \n",
    "def demo():\n",
    "    # Create a node\n",
    "    rospy.init_node(\"face_recognition_demo_node\")\n",
    "    \n",
    "    # Make sure sim time is working\n",
    "    while not rospy.Time.now():\n",
    "        pass\n",
    "    \n",
    "    move_base_pub = MoveBasePublisher()\n",
    "    rospy.loginfo(\"Start Sequence complete\")\n",
    "    rospy.loginfo(\"Move to position ...\")\n",
    "    move_base_pub.move_forwards_backwards(speed_ms=0.3, position_x=1.0, allowed_error=0.1)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    demo()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d and 3d Object Finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basic approach for detecting objects, although you can already differenciate objects and localise them. For this you will use \"find-object-2d\" package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to get closer to the table to see the objects as close as possible to make better detections. You can use your recently created script to move the Fecth robot closer to the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2d Object Finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have an exmaple of the launch you would have to launch to start the basic system:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> start_find_object_2d.launch   </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "\n",
    "<launch>\n",
    "    <arg name=\"camera_rgb_topic\" default=\"/head_camera/rgb/image_raw\" />\n",
    "    <node name=\"find_object_2d_node\" pkg=\"find_object_2d\" type=\"find_object_2d\" output=\"screen\">\n",
    "        <remap from=\"image\" to=\"$(arg camera_rgb_topic)\"/>\n",
    "    </node>\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> END   </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have only to set the RGB camera image souce and the system is ready to go. In this case its **/head_camera/rgb/Image_raw**. Once you launch it you should go to The Graphical Tools and see something similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/R3.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! Now You can add as many objects as you want, or even turn the object around and take images from different points of view. This will make it detect the object all the time, only bare in mind that without the proper filtering, the system will consider it as different objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the last step is to save all the objects added. There are mainly 2 ways:\n",
    "* Saving the Objects as images: File --> Save_Objects. This will save in a folder all the images taken\n",
    "* Saving the Whole session: File -->Save_Session. This will save a binary with all the images and settings. This is the most compact way of doing it, although you wont have acces to the images of the objects. Depends on your needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save the object images directly by rightclicking on them but <font color=red> BEWARE </font> that if you do so, these images can't be used for later delections, because they aren't mirrored, therefore the recognitin wont work. You can see an example of it here, its the sam image but one is the manualy saved version that is not mirrored and isn't detected, while the other one is correctly saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So once you have your session or your images stored, you need to be able to always start an object recognition session with all that stored data. To do so you have the folloeing options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For saved sessions in my_object_revognition_pkg, in the directory saved_pictures2d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<launch>\n",
    "    <arg name=\"camera_rgb_topic\" default=\"/head_camera/rgb/image_raw\" />\n",
    "    <!-- Nodes -->\n",
    "    <node name=\"find_object_2d_node\" pkg=\"find_object_2d\" type=\"find_object_2d\" output=\"screen\">\n",
    "        <remap from=\"image\" to=\"$(arg camera_rgb_topic)\"/>\n",
    "        <param name=\"gui\" value=\"true\" type=\"bool\"/>\n",
    "        <param name=\"session_path\" value=\"$(find my_object_recongnition_pkg)/saved_pictures2d/coke_session.bin\" type=\"str\"/>\n",
    "        <param name=\"settings_path\" value=\"~/.ros/find_object_2d.ini\" type=\"str\"/>\n",
    "    </node>\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For saved images in my_object_recognition_pkg, in the directory saved_pictures2d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<launch>\n",
    "    <arg name=\"camera_rgb_topic\" default=\"/head_camera/rgb/image_raw\" />\n",
    "    <!-- Nodes -->\n",
    "    <node name=\"find_object_2d_node\" pkg=\"find_object_2d\" type=\"find_object_2d\" output=\"screen\">\n",
    "        <remap from=\"image\" to=\"$(arg camera_rgb_topic)\"/>\n",
    "        <param name=\"gui\" value=\"true\" type=\"bool\"/>\n",
    "        <param name=\"objects_path\" value=\"$(find my_object_recongnition_pkg)/saved_pictures2d/coke_session.bin\" type=\"str\"/>\n",
    "        <param name=\"settings_path\" value=\"~/.ros/find_object_2d.ini\" type=\"str\"/>\n",
    "    </node>\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise U3-2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a launch file that starts the 2d object recognition through a saved seddion where you have saved the coke can object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Move and Spawn objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to test your object_recognition system with the same object in different positions or even in movement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also need to test it with various objects in scene to be sure that it doesn't mistake a human head with a coke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you are going to learn how to move around objects in a Gazebo scene, and spawn new ones also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make an object move in a scene there are two steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Make the object movable in Gazebo**: for that you wil use the move_model.launch from our spawn_robot_tools_pkgs. THis makes available a topic called \"name_object/cmd_vel\" on which you can then publish and move the model around.\n",
    "* **Publish in the correct topic** to move the object through the keyboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know exactly how this works, see TF and URDF Tutorium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> make_cokecan_moveable.launch</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "\n",
    "<launch>\n",
    "    <arg name=\"robot_name\" default=\"coke_can\" />\n",
    "    \n",
    "    <include file=\"$(find spawn_robot_tools_pkg)/launch/move_model.launch\">\n",
    "        <arg name=\"robot_name\" value=\"$(arg robot_name)\">\n",
    "    </include>\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> End   </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> MOVE_COKE_KEYBOARD.LAUNCH   </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "\n",
    "<launch>\n",
    "    <arg name=\"robot_name\" default=\"coke_can\" />\n",
    "    <node name=\"$(arg robot_name)_twist_keyboard\" pkg=\"spwan_robot_tools_pkg\" type=\"model_twist_keyboard.py\" \n",
    "    args=\"$(arg robot_name)\" output=\"screen\"/>\n",
    "</launch>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> End  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To move other objects in the scence just change the name of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know the name of the model has in Gazebo you can ask the gazebo service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Execute in WebShell #1\n",
    "\n",
    "rosservice call /gazebo/get_world_properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now how do you spawn new object in the scene?\n",
    "<br>\n",
    "There are many ways, ***we highly recommend you to learn TF and URDF robot creation***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method needs you to have the sdf files of the models on your package and the whole model installed in the .gazebo/models path. For simplicity we have granted access to three of them: beer, coke_can and hammer. You really only need the sdf files of each model, the rest will be retrieved from the gazebo path, not your package. You can copy them to your package by:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Execute in WebSchell #1\n",
    "\n",
    "roscd object_recogn_tc\n",
    "cp models -f /home/user/catkin_ws/my_object_recognition_pkg/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have them just execute the following launch file to spawn the model where you wish, probably in the table location.\n",
    "<br>\n",
    "For Reference, the coke_can is spawned in the center of the table at XYZ=(-2.0, 0.0, 0.8).\n",
    "<br>\n",
    "Bare in mind that if there is already an existing model with the same name, you wont be able to spawn it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> spawn_coke.launch   </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<launch>\n",
    "\n",
    "    <arg name=\"sdf_robot_file\" value=\"$(find my_object_recognition_pkg)/models/coke_can/model.sdf\" />\n",
    "    <arg name=\"robot_name\" value=\"coke_can\" />\n",
    "    <arg name=\"x\" default=\"-2.0\" />\n",
    "    <arg name=\"y\" default=\"0.2\" />\n",
    "    <arg name=\"z\" default=\"0.8\" />\n",
    "    <arg name=\"yaw\" default=\"0.0\" />\n",
    "    \n",
    "    <include file=\"$(find spawn_robot_tools_pkg)/launch/spawn_sdf.launch\">\n",
    "        <arg name=\"sdf_robot_file\" value=\"$(arg sdf_robot_file)\" />\n",
    "        <arg name=\"robot_name\" default=\"$(arg robot_name)\" />\n",
    "        <arg name=\"x\" default=\"$(arg x)\" />\n",
    "        <arg name=\"y\" default=\"$(arg y)\" />\n",
    "        <arg name=\"z\" default=\"$(arg z)\" />\n",
    "        <arg name=\"yaw\" default=\"$(arg yaw)\" />\n",
    "    </include>\n",
    "    \n",
    "</launch>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> End   </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in conclusion to spawn an object, for example the coke_can, and make it mobable at the same time you have to execute this launch file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> spawn_ready_coke.lauch   </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<launch>\n",
    "    <!-- Spawn CokeCan Model -->\n",
    "    <include file=\"$(find my_object_recognition_pkg)/launch/spwan_coke.launch\" />\n",
    "    <!-- Make CokeCan Movable -->\n",
    "    <include file=\"$(find my_object_recognition_pkg)/launch/make_cokecan_movable.launch\" />\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> End   </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And afterwards in another terminal, execute the **move_coke_keyboard.launch**."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Execute in WebShell #1\n",
    "\n",
    "roslaunch my_object_recognition_pkg spawn_ready_coke.launch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Execute in WebShell #2\n",
    "\n",
    "roslaunch my_object_recognition_pkg smove_coke_keyboard.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to delete the object, just RIGHTCLICK ON it and Delete it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> Exercise U3-3  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the following:\n",
    "* Delete the existing cocke model of the scene.\n",
    "* Spawn a new coke_can on the table.\n",
    "* Move the coke can around and see how robust is the detection system with only one picture.\n",
    "* Then make as many pictures as you need to never losse track of the object. The best method is to pivot in place the coke can until it stops detecting. Then add it as a new object, and repeat the process until it detects it 360 degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> End  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should at the end have a detection squeme similar to this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/R4.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> Help Exercise U3-4   </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same as in Exercise U3-3 but wih a beer can and a hammer.\n",
    "<br>\n",
    "For this you will have to spawn and move them first.\n",
    "<br>\n",
    "Remmember to save the session at the end so that you dont losse all your object images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> End  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d Object Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now the only thing missing is having the position of the objects in 3d space to be able to grasp them. To know how to grasp an object, please do the *Manipulation course*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only real difference with the 2d detection will be the sensors involved and the fact that the ObjectPoseStamped will be transformed into TFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"> **Warming** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to create another round of session fotos in the 3d system, because otherwise the detection wont work as well as they should. Specially for the TF transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> start_find_object_3d_session.launch   </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<launch>\n",
    "    \n",
    "    <node name=\"find_object_3d\" pkg=\"find_object_3d\" type=\"find_object_3d\" output=\"screen\">\n",
    "        <param name=\"gui\" value=\"true\" type=\"bool\" />\n",
    "        <param name=\"settings_path\" value=\"~/.ros/find_object_2d.ini\" type=\"str\" />\n",
    "        <param name=\"subscribe_depth\" value=\"true\" type=\"bool\" />\n",
    "        <param name=\"session_path\" value=\"$(find my_object_recognition_pkg)/saved_pictures2d/coke_session.bin\"\n",
    "        type=\"str\" />\n",
    "        <param name=\"objects_path\" value=\"\" type=\"str\" />\n",
    "        <param name=\"object_prefix\" value=\"object\" type=\"str\" />\n",
    "        \n",
    "        <remap from=\"rgb/image_rect_color\" to=\"/head_camera/rgb/image_raw\" />\n",
    "        <remap from=\"depth_registered/image_raw\" to=\"/head_camera/depth_registered/camera_info\" />\n",
    "    </node>\n",
    "    \n",
    "    <!-- Example of tf synchronisation with the objectsStamped message -->\n",
    "    <node name=\"tf_example\" pkg=\"find_object_2d\" type=\"tf_example\" output=\"screen\" />\n",
    "        <param name=\"map_frame_id\" value=\"/map\" type=\"string\" />\n",
    "        <param name=\"object_prefix\" value=\"object\" type=\"str\" />\n",
    "    </node>\n",
    "    \n",
    "</launch>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: green\"> End   </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching this you should then get the TF published of the object detected. If you have multiple images of the same object you will get multiple frame of objects, that's up to you to filter them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see the object detected by executed by executing in another terminal while the prior launch is working, the following command:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Execute in WebShell  #2\n",
    "\n",
    "rosrun find_object_2d print_objects_detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> Exercise U3-4   </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the following:\n",
    "* Launch the 3d object detector with a session with only one image per object. This way you wont get a clutter of TF frames.\n",
    "* Move the objects around through a python script. This way you will be able to see how the TFs change and how accurate they are in relation to the real position of the objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> End</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TFs appearing can be lowered by lowering the time you sonsifer a TF obsolete. Because most of these TFS are from previous detections that you stay there for a while until they are old enough to consider them irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> Extra Exercise U3-5   </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the following: \n",
    "* Spawn two objects in the table and make them move on the table.\n",
    "* Detect one of the objects.\n",
    "* Track that object and make the Fetch Robot track it with the head to keep the object in the center of the view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: white; background-color: orange\"> End   </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
